{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import yaml\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('VCDI')\n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "sys.path.insert(0,'../models/')\n",
    "from utils import generate_model_config, generate_gridsearch_configs, train_model\n",
    "\n",
    "sys.path.insert(0,'/anaconda3/envs/model_search/lib/python3.7/site-packages') #This is specific to my environment in order to import xgboost, proably not needed in others\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = 'regression' #classification or regression\n",
    "experiment_folder = sorted(glob.glob(f'../results/{train_type}/20*'))[-1]\n",
    "last_results = experiment_folder+ '/results_df.pkl'\n",
    "results_df = pd.read_pickle(last_results)\n",
    "\n",
    "with open(f\"{experiment_folder}/config_{train_type}.yaml\", 'r') as stream:\n",
    "    config_experiment = yaml.load(stream)\n",
    "\n",
    "if 'class_weight' in results_df.columns:\n",
    "    results_df['class_weight'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metric names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [key for key in config_experiment['metrics']if config_experiment['metrics'][key]]\n",
    "metrics = [m+'_train' for m in metrics] + [m+'_val' for m in metrics]\n",
    "print('Recorded metrics: ')\n",
    "for metric in metrics:\n",
    "    print(f'   {metric}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average over cross val folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossfold_df = results_df.groupby(['model_num'], sort=False)[metrics].mean()\n",
    "model_params = results_df[results_df['fold']==0].drop(crossfold_df.columns, axis=1).reset_index(drop=True)\n",
    "crossfold_df = pd.concat([model_params, crossfold_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise best results for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'mean_squared_error_val'\n",
    "idx = crossfold_df.groupby(['model'])[metric].transform(min) == crossfold_df[metric]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x=\"model\", y=metric, data=crossfold_df[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick best model and look into parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'mean_squared_error_val'\n",
    "best_type = crossfold_df.loc[crossfold_df[metric].idxmin(),'model'] # type of best model   any(x in str for x in a)\n",
    "tested_params = model_params[model_params.loc[:,'model']==best_type].loc[:,[column for column in model_params.columns if not any(x in column for x in ['label_count', 'train_time', 'model_num'])]].dropna(axis=1, how='all')\n",
    "\n",
    "print(f'The best model type for {metric} was: {best_type}\\n')\n",
    "print('The following parameters were tested:')\n",
    "for col in tested_params.columns:\n",
    "    if len(tested_params[col].unique())>1:\n",
    "        print(f'    {col}: {tested_params[col].unique()}')\n",
    "        \n",
    "print('\\nThe best model was:')\n",
    "for col in tested_params.columns:\n",
    "    if len(tested_params[col].unique())>1:\n",
    "        print(f'    {col}: {tested_params.loc[crossfold_df[metric].idxmin(), col]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter1 = 'n_estimators'\n",
    "parameter2 = 'max_features'\n",
    "metric = 'mean_squared_error_val'\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.catplot(x=parameter1, y=metric, hue=parameter2, data=crossfold_df[crossfold_df.loc[:,'model']==best_type], kind=\"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate effect of model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter1 = 'n_estimators'\n",
    "param_dict = {\n",
    "    #'max_features': 0.1, \n",
    "    #'n_estimators': 200,\n",
    "}\n",
    "metric = 'mean_squared_error'\n",
    "plot_df = crossfold_df[crossfold_df.loc[:,'model']==best_type]\n",
    "for key in param_dict:\n",
    "    plot_df = plot_df[plot_df.loc[:,key]==param_dict[key]]\n",
    "\n",
    "df_tmp =  pd.melt(plot_df, id_vars=parameter1, value_vars=[metric+pred_type for pred_type in ['_train', '_val']], var_name=\"metric type\", value_name=metric)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.catplot(x=parameter1, y=metric, hue=\"metric type\", data=df_tmp, kind=\"point\")\n",
    "plt.ylabel(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
